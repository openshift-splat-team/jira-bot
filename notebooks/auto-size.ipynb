{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PoC which attempts to size issues based on our team's sizing history. To train the model, export issues as CSV from Jira which have a sizing in \"Custom field (Story Points)\". The more issues the better.  Save the CSV as `training_issues.csv`.  Similarly, export issues which lack a sizing as a CSV named `unsized_issues.csv`. This notebook will train on issues based on their summary, description, and issue type. Predictions are made based on 13 categories and differentiated with softmax.\n",
    "\n",
    "Note: Sizings are a highly variable activity from team to team. Each team would need to train their model based on issues in their project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"training_issues.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.is_punct and not token.is_stop]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def getVectorized(col, df_in):\n",
    "    vectorizer = TfidfVectorizer()        \n",
    "    matrix = vectorizer.fit_transform(df_in[col].apply(preprocess_text))    \n",
    "    return pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "def prepareDataframe(df_in):\n",
    "    df_work = df_in[[\"Description\", \"Summary\", \"Issue Type\", \"Custom field (Story Points)\"]]    \n",
    "    df_work[\"Description\"] = df_work[\"Description\"].fillna(\"\")\n",
    "    df_work['issue_text'] = df_work[\"Summary\"] + \" \" + df_work[\"Description\"]\n",
    "    df_vect = getVectorized(\"issue_text\", df_work)    \n",
    "    df_cat = pd.get_dummies(df_work, columns=['Issue Type'])            \n",
    "    df_cat.drop([\"Description\", \"Summary\", \"issue_text\"], axis=1, inplace=True)    \n",
    "    df_out = pd.concat([df_cat, df_vect], axis=1)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "df_full = prepareDataframe(df)\n",
    "\n",
    "x_cols = df_full.columns[df_full.columns != \"Custom field (Story Points)\"]\n",
    "\n",
    "x = df_full[x_cols].values\n",
    "df_full[\"Custom field (Story Points)\"] = df_full[\"Custom field (Story Points)\"].astype(\"float\")\n",
    "y = df_full[\"Custom field (Story Points)\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
    "\n",
    "num_classes = 14\n",
    "y_train_encoded = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_encoded = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import backend\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#np.random.seed(42)\n",
    "\n",
    "#import random\n",
    "#random.seed(42)\n",
    "\n",
    "#tf.random.set_seed(42)\n",
    "\n",
    "backend.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(14, activation='softmax'))\n",
    "\n",
    "opt = Adamax()\n",
    "\n",
    "model.compile(optimizer=opt, loss='categorical_focal_crossentropy', metrics=['accuracy'])  # Change loss function for regression/multi-class\n",
    "\n",
    "# Train the model\n",
    "training_stats = model.fit(X_train, y_train_encoded, epochs=100, batch_size=32, validation_data=(X_test, y_test_encoded))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "plt.plot(training_stats.history['accuracy'])\n",
    "plt.plot(training_stats.history['val_accuracy'])\n",
    "plt.title('Accuracy vs Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = pd.read_csv(\"unsized_issues.csv\")\n",
    "df_input_prepared = prepareDataframe(df_input)\n",
    "\n",
    "# Identify missing features\n",
    "missing_features = [feature for feature in df_full.columns if feature not in df_input_prepared.columns]\n",
    "\n",
    "# Create a DataFrame for missing features with default values (e.g., 0)\n",
    "missing_data = pd.DataFrame(0, index=df_input_prepared.index, columns=missing_features)\n",
    "\n",
    "# Concatenate the original new data with the missing features DataFrame\n",
    "new_data_complete = pd.concat([df_input_prepared, missing_data], axis=1)\n",
    "\n",
    "# Reorder columns to match the expected feature order\n",
    "new_data_complete = new_data_complete[df_full.columns]\n",
    "\n",
    "new_data_complete.drop([\"Custom field (Story Points)\"], axis=1, inplace=True)\n",
    "\n",
    "print(new_data_complete.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(new_data_complete)\n",
    "new_data_scaled = scaler.transform(new_data_complete)\n",
    "predictions = model.predict(new_data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "scores = []\n",
    "for prediction in predictions:\n",
    "    scores.append(np.argmax(prediction))\n",
    "\n",
    "score_df = pd.DataFrame(scores, columns=[\"sizing\"])\n",
    "output_df = pd.concat([df_input, score_df], axis=1)\n",
    "output_df = output_df.dropna(axis=1, how='all')\n",
    "output_df.to_csv(\"auto_sized.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
